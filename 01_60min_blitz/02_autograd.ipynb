{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd: automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`autograd` is pytorch's automatic differentiation package, and is defined by how the code is run, not by a static graph like tensorflow (though Eager Mode in TF2 changes things)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import itertools as it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors initialized with data do not have their gradients computed automatically\n",
    "x = torch.tensor([2, 2])\n",
    "print(x)\n",
    "x.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7ff7786ee810>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Because y was created as a result of an operation with a tensor\n",
    "# whose gradient is being tracked, it has a function associated with it\n",
    "\n",
    "print(y.grad_fn)\n",
    "# The tensor we created does not\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Some more ops\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "<SumBackward0 object at 0x7ff7e84a1d90>\n"
     ]
    }
   ],
   "source": [
    "# .requires_grad_() changes a tensor's requires_grad flag in-place (note the\n",
    "# post-fixed '_')\n",
    "# Should probably not set the attribute directly...\n",
    "\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# backprop through a single scalar using .backward() method\n",
    "print(out)\n",
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n",
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "# This computed the gradient of `out` wrt every tensor in the computational\n",
    "# graph\n",
    "# remember y = x + 2, z = y * y * 3, and out = z.mean()\n",
    "# Since out is a scalar and x is a (2, 2) tensor, that means the gradient\n",
    "# d(out)/dx is a (2, 2) tensor as well\n",
    "\n",
    "print(x)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1111.8179,  434.1864, 1381.8116], grad_fn=<MulBackward0>)\n",
      "\n",
      "tensor([[1024.,    0.,    0.],\n",
      "        [   0., 1024.,    0.],\n",
      "        [   0.,    0., 1024.]])\n"
     ]
    }
   ],
   "source": [
    "# If we take a gradient of a non-scalar tensor, the dimension changes\n",
    "# For example, the gradient of a (3,) tensor (aka a vector) with another (3,)\n",
    "# tensor is a (3, 3) tensor\n",
    "\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "y = 2 * x\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y, end='\\n\\n')\n",
    "\n",
    "J = list()\n",
    "for i in range(y.shape[0]):\n",
    "    y[i].backward(retain_graph=True)\n",
    "    J.append(x.grad.clone())\n",
    "    x.grad.zero_() # Need to zero out the gradient each time\n",
    "                   # because it is accumulated by default\n",
    "print(torch.stack(J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the above Jacobian is proportional to the identity matrix, which makes sense because $y_i = 2^n x_i$ and doesn't depend on any of the other elements of $\\vec{x}$, so $dy_i/dx_j = 0$ for $i \\neq j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "# Dot product of gradient with the vector v\n",
    "x.grad.zero_() # have to zero the gradient from the \n",
    "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
    "y.backward(v, retain_graph=True)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# You can stop autograd from tracking history on Tensors with a context manager\n",
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# or we can get a copy of the tensor that does not require gradients\n",
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "pytorch-tutorial",
   "language": "python",
   "name": "pytorch-tutorial"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
